{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "magnet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Ho_ipTkMmF8PCeacWY5ml7rO-bhgt25J",
      "authorship_tag": "ABX9TyNOCmvvvVgGdZSl7wIRatcK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CastleJin/2021_01_12_winter_internship_magnet/blob/main/magnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LRGW6ubYGXX"
      },
      "source": [
        "**Set** **Prerequisite**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGdFOExPgvpo"
      },
      "source": [
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzEEQds9OYvz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o-P-FkMXM8G"
      },
      "source": [
        "!pip install Pillow==7.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzBWaccFEPmX"
      },
      "source": [
        "!python -V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csst9VyeoB4w"
      },
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxksfzpoTVuw"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCo82GBpTdlq"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import math\r\n",
        "\r\n",
        "# modules\r\n",
        "class res_blk(nn.Module):\r\n",
        "  def __init__(self, layer_dims, ks, s):\r\n",
        "    super(res_blk, self).__init__()\r\n",
        "    p = int((ks - 1) / 2)\r\n",
        "    self.conv1 = nn.Conv2d(layer_dims, layer_dims, kernel_size=ks, stride=s, padding=p, padding_mode='reflect', bias=False)\r\n",
        "    self.activation = nn.ReLU()\r\n",
        "\r\n",
        "  def forward(self, input):\r\n",
        "    out = self.conv1(input)\r\n",
        "    out = self.activation(out)\r\n",
        "    out = self.conv1(out)\r\n",
        "    return input + out\r\n",
        "\r\n",
        "def multi_res_blk(num_res_blk, layer_dims, ks, s):\r\n",
        "  layers = []\r\n",
        "  for i in range(num_res_blk):\r\n",
        "    layers.append(res_blk(layer_dims, ks, s))\r\n",
        "  return nn.Sequential(*layers)\r\n",
        "\r\n",
        "class res_manipulator(nn.Module):\r\n",
        "  def __init__(self, layer_dims=32):\r\n",
        "    super(res_manipulator, self).__init__()\r\n",
        "    self.conv1 = nn.Conv2d(layer_dims, layer_dims, kernel_size=7, stride=1, padding=3, padding_mode='reflect', bias=False)\r\n",
        "    self.conv2 = nn.Conv2d(layer_dims, layer_dims, kernel_size=3, stride=1, padding=1, padding_mode='reflect', bias=False) \r\n",
        "    self.residual = multi_res_blk(1, layer_dims, 3, 1)\r\n",
        "    self.activation = nn.ReLU()\r\n",
        "\r\n",
        "  def forward(self, enc_a, enc_b, amp_factor):\r\n",
        "    out = enc_b - enc_a\r\n",
        "    out = self.activation(self.conv1(out))\r\n",
        "    out *= amp_factor\r\n",
        "    out = self.conv2(out)\r\n",
        "    out = self.residual(out)\r\n",
        "    return enc_b + out\r\n",
        "\r\n",
        "class res_encoder(nn.Module):\r\n",
        "  def __init__(self, layer_dims=32, num_res_blk=3):\r\n",
        "    super(res_encoder, self).__init__()\r\n",
        "    self.conv1 = nn.Conv2d(3, int(layer_dims / 2), kernel_size = 7, stride = 1, padding = 3, padding_mode = 'reflect', bias=False)\r\n",
        "    self.conv2 = nn.Conv2d(int(layer_dims / 2), layer_dims, kernel_size = 3, stride = 2, padding = 1, padding_mode = 'reflect',bias=False)\r\n",
        "    self.residual = multi_res_blk(num_res_blk, layer_dims, 3, 1)\r\n",
        "    self.activation = nn.ReLU()\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    out = self.activation(self.conv1(x))\r\n",
        "    out = self.activation(self.conv2(out))\r\n",
        "    out = self.residual(out)\r\n",
        "    return out\r\n",
        "\r\n",
        "class res_decoder(nn.Module):\r\n",
        "  def __init__(self, layer_dims=64, num_res_blk=9):\r\n",
        "    super(res_decoder, self).__init__()\r\n",
        "    self.residual = multi_res_blk(num_res_blk, layer_dims, 3, 1)\r\n",
        "    self.up_sample = nn.Upsample(scale_factor = 2, mode = 'nearest')\r\n",
        "    self.conv1 = nn.Conv2d(layer_dims, int(layer_dims / 2), kernel_size = 3, stride = 1, padding = 1, padding_mode = 'reflect', bias=False) ## change\r\n",
        "    self.conv2 = nn.Conv2d(int(layer_dims / 2), 3, kernel_size = 7, stride = 1, padding = 3, padding_mode = 'reflect', bias=False) ## change\r\n",
        "    self.activation = nn.ReLU()\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    out = self.residual(x)\r\n",
        "    out = self.up_sample(out)\r\n",
        "    out = self.activation(self.conv1(out))\r\n",
        "    out = self.conv2(out)\r\n",
        "    return out\r\n",
        "\r\n",
        "# magnet\r\n",
        "class encoder(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(encoder, self).__init__()\r\n",
        "    # set variables\r\n",
        "    self.num_enc_resblk = 3\r\n",
        "    self.res_enc_dim = 32\r\n",
        "    self.num_texture_resblk = 2\r\n",
        "    self.num_shape_resblk = 2\r\n",
        "\r\n",
        "    # set arch\r\n",
        "    self.res_encoder = res_encoder(self.res_enc_dim ,self.num_enc_resblk)\r\n",
        "    self.conv_tex = nn.Conv2d(self.res_enc_dim, self.res_enc_dim, kernel_size = 3, stride = 2, padding = 1, padding_mode = 'reflect', bias=False) # stride is 2, cause texture_downsample is True, else 1\r\n",
        "    self.conv_sha = nn.Conv2d(self.res_enc_dim, self.res_enc_dim, kernel_size = 3, stride = 1, padding = 1, padding_mode = 'reflect', bias=False)\r\n",
        "    self.texture_resblk = multi_res_blk(self.num_texture_resblk, self.res_enc_dim, 3, 1)\r\n",
        "    self.shape_resblk = multi_res_blk(self.num_shape_resblk, self.res_enc_dim, 3, 1)\r\n",
        "    self.activation = nn.ReLU()\r\n",
        "\r\n",
        "  def forward(self, img):\r\n",
        "    enc = self.res_encoder(img)\r\n",
        "    texture_enc = enc\r\n",
        "    shape_enc = enc\r\n",
        "    texture_enc = self.activation(self.conv_tex(texture_enc))\r\n",
        "    texture_enc = self.texture_resblk(texture_enc)\r\n",
        "    shape_enc = self.activation(self.conv_sha(shape_enc))\r\n",
        "    shape_enc = self.shape_resblk(shape_enc)\r\n",
        "    return texture_enc, shape_enc\r\n",
        "\r\n",
        "class decoder(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(decoder, self).__init__()\r\n",
        "    # set variables\r\n",
        "    self.num_dec_resblk = 9\r\n",
        "    self.texture_dims = 32\r\n",
        "    self.shape_dims = 32\r\n",
        "    self.decoder_dims = self.texture_dims + self.shape_dims\r\n",
        "    \r\n",
        "    # set arch\r\n",
        "    self.up_sample = nn.Upsample(scale_factor = 2, mode = 'nearest') # texture가 downsampling 됐을 때 activate한다.\r\n",
        "    self.conv_tex_aft_upsample = nn.Conv2d(self.texture_dims, self.texture_dims, kernel_size = 3, stride = 1, padding = 1, padding_mode = 'reflect', bias=False)\r\n",
        "    self.res_decoder = res_decoder(self.decoder_dims, self.num_dec_resblk)\r\n",
        "    self.activation = nn.ReLU()\r\n",
        "  \r\n",
        "  def forward(self, texture_enc, shape_enc):\r\n",
        "    texture_enc = self.up_sample(texture_enc) # texture가 downsampling 됐을 때 activate한다.\r\n",
        "    texture_enc = self.activation(self.conv_tex_aft_upsample(texture_enc))\r\n",
        "    enc = torch.cat((texture_enc, shape_enc), 1)\r\n",
        "    return self.res_decoder(enc)\r\n",
        "\r\n",
        "class magnet(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(magnet, self).__init__()\r\n",
        "    self.encoder = encoder()\r\n",
        "    self.decoder = decoder()\r\n",
        "    self.res_manipulator = res_manipulator()\r\n",
        "\r\n",
        "  def forward(self, amplified, image_a, image_b, image_c, amp_factor):\r\n",
        "    texture_amp, _ = self.encoder(amplified)\r\n",
        "    texture_a, shape_a = self.encoder(image_a)\r\n",
        "    texture_b, shape_b = self.encoder(image_b)\r\n",
        "    texture_c, shape_c = self.encoder(image_c)\r\n",
        "    out_shape_enc = self.res_manipulator(shape_a, shape_b, amp_factor)\r\n",
        "    out = self.decoder(texture_b, out_shape_enc)\r\n",
        "\r\n",
        "    return out, texture_a, texture_c, texture_b, texture_amp, shape_b, shape_c\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfa7FjQeThq1"
      },
      "source": [
        "**Dataset & Dataloader Setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtIPWmvPTpAO"
      },
      "source": [
        "# References\r\n",
        "# 1. https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\r\n",
        "# 2. https://wikidocs.net/57165\r\n",
        "# 3. https://pytorch.org/docs/master/_modules/torch/utils/data/sampler.html#Sampler\r\n",
        "\r\n",
        "import os\r\n",
        "from PIL import Image\r\n",
        "from __future__ import print_function\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "from matplotlib.pyplot import imshow\r\n",
        "import random\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import transforms\r\n",
        "from torch.utils.data.sampler import Sampler\r\n",
        "\r\n",
        "class three(Dataset):\r\n",
        "  def __init__(self, data_path, transform = None):\r\n",
        "    self.data_path = data_path # root of dataset\r\n",
        "    self.sub_dir = os.listdir(self.data_path) # sub directory\r\n",
        "    \r\n",
        "    # data path\r\n",
        "    self.amplified_path = os.path.join(self.data_path, self.sub_dir[0])\r\n",
        "    self.frameA_path = os.path.join(self.data_path, self.sub_dir[1])\r\n",
        "    self.frameB_path = os.path.join(self.data_path, self.sub_dir[2])\r\n",
        "    self.frameC_path = os.path.join(self.data_path, self.sub_dir[3])\r\n",
        "    self.meta_path = os.path.join(self.data_path, self.sub_dir[4])\r\n",
        "    self.transform = transform\r\n",
        "\r\n",
        "  \r\n",
        "  def _read_json(self, path):\r\n",
        "    with open(path,'r') as f:\r\n",
        "      json_data = json.load(f)\r\n",
        "    return json_data['amplification_factor']\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    file_list = os.listdir(self.amplified_path)\r\n",
        "    return len(file_list)\r\n",
        "  \r\n",
        "  def __getitem__(self,idx):\r\n",
        "    # subfile list\r\n",
        "    amplified_list = os.listdir(self.amplified_path)\r\n",
        "    frameA_list = os.listdir(self.frameA_path)\r\n",
        "    frameB_list = os.listdir(self.frameB_path)\r\n",
        "    frameC_list = os.listdir(self.frameC_path)\r\n",
        "    meta_list = os.listdir(self.meta_path)\r\n",
        "\r\n",
        "    # sort\r\n",
        "    amplified_list.sort()\r\n",
        "    frameA_list.sort()\r\n",
        "    frameB_list.sort()\r\n",
        "    frameC_list.sort()\r\n",
        "    meta_list.sort()\r\n",
        "  \r\n",
        "    # read image & json\r\n",
        "    amplified = Image.open(os.path.join(self.amplified_path, amplified_list[idx]))\r\n",
        "    frameA = Image.open(os.path.join(self.frameA_path, frameA_list[idx]))\r\n",
        "    frameB = Image.open(os.path.join(self.frameB_path, frameB_list[idx]))\r\n",
        "    frameC = Image.open(os.path.join(self.frameC_path, frameC_list[idx]))\r\n",
        "    mag_factor = self._read_json(os.path.join(self.meta_path, meta_list[idx]))\r\n",
        "\r\n",
        "    # convert nparray & normalize to -1 to 1\r\n",
        "    amplified = np.array(amplified, dtype = 'float32') / 127.5 - 1.0\r\n",
        "    frameA = np.array(frameA, dtype = 'float32') / 127.5 - 1.0\r\n",
        "    frameB = np.array(frameB, dtype = 'float32') / 127.5 - 1.0\r\n",
        "    frameC = np.array(frameC, dtype = 'float32') / 127.5 - 1.0\r\n",
        "    mag_factor -= 1.0\r\n",
        "    mag_factor = np.array(mag_factor, dtype = 'float32')\r\n",
        "\r\n",
        "    sample = {'amplified': amplified, 'frameA': frameA, 'frameB': frameB, 'frameC': frameC, 'mag_factor': mag_factor}\r\n",
        "\r\n",
        "    if self.transform is not None:\r\n",
        "      sample = self.transform(sample)\r\n",
        "\r\n",
        "    return sample\r\n",
        "\r\n",
        "class ToTensor(object):\r\n",
        "  def __call__(self, sample):\r\n",
        "    amplified, frameA, frameB, frameC, mag_factor = sample['amplified'], sample['frameA'], sample['frameB'], sample['frameC'], sample['mag_factor']\r\n",
        "    # swap color axis because\r\n",
        "    # numpy image: H x W x C\r\n",
        "    # torch image: C X H X W\r\n",
        "    amplified = amplified.transpose((2, 0, 1))\r\n",
        "    frameA = frameA.transpose((2, 0, 1))\r\n",
        "    frameB = frameB.transpose((2, 0, 1))\r\n",
        "    frameC = frameC.transpose((2, 0, 1))\r\n",
        "\r\n",
        "    # convert tensor\r\n",
        "    amplified = torch.from_numpy(amplified)\r\n",
        "    frameA = torch.from_numpy(frameA)\r\n",
        "    frameB = torch.from_numpy(frameB)\r\n",
        "    frameC = torch.from_numpy(frameC)\r\n",
        "    mag_factor = torch.from_numpy(mag_factor)\r\n",
        "    mag_factor = mag_factor.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\r\n",
        "\r\n",
        "    ToTensor_sample = {'amplified': amplified, 'frameA': frameA, 'frameB': frameB, 'frameC': frameC, 'mag_factor': mag_factor}\r\n",
        "    return ToTensor_sample\r\n",
        "\r\n",
        "class shot_noise(object):\r\n",
        "  # This function approximate poisson noise upto 2nd order.\r\n",
        "  def __init__(self, n):\r\n",
        "    self.n = n\r\n",
        "\r\n",
        "  def _get_shot_noise(self, image):\r\n",
        "    n = torch.zeros_like(image).normal_(mean=0.0, std=1.0)\r\n",
        "    # strength ~ sqrt image value in 255, divided by 127.5 to convert\r\n",
        "    # back to -1, 1 range.\r\n",
        "    # 그러나 strength ~ sqrt image value in 255가 이해가 되지 않음.\r\n",
        "    # 제곱에 비례해야하는 것이 아닌가?\r\n",
        "\r\n",
        "    n_str = torch.sqrt(torch.as_tensor(image + 1.0)) / torch.sqrt(torch.as_tensor(127.5))\r\n",
        "    return torch.mul(n, n_str)\r\n",
        "\r\n",
        "  def _preproc_shot_noise(self, image, n):\r\n",
        "    nn = np.random.uniform(0, n)\r\n",
        "    return image + nn * self._get_shot_noise(image)\r\n",
        "\r\n",
        "  def __call__(self, sample):\r\n",
        "    amplified, frameA, frameB, frameC, mag_factor = sample['amplified'], sample['frameA'], sample['frameB'], sample['frameC'], sample['mag_factor']\r\n",
        "    # add shot noise\r\n",
        "    frameA = self._preproc_shot_noise(frameA, self.n)\r\n",
        "    frameB = self._preproc_shot_noise(frameB, self.n)\r\n",
        "    frameC = self._preproc_shot_noise(frameC, self.n)\r\n",
        "\r\n",
        "    preproc_sample = {'amplified': amplified, 'frameA': frameA, 'frameB': frameB, 'frameC': frameC, 'mag_factor': mag_factor}\r\n",
        "    return preproc_sample\r\n",
        "\r\n",
        "class num_sampler(Sampler):\r\n",
        "# Sampling a specific number of multiple-th indices from data.\r\n",
        "  def __init__(self, data, is_val=True, shuffle=False, num=10):\r\n",
        "    self.num_samples = len(data)\r\n",
        "    self.is_val = is_val\r\n",
        "    self.shuffle = shuffle\r\n",
        "    self.num = num\r\n",
        "\r\n",
        "  def __iter__(self):\r\n",
        "    k = []\r\n",
        "    for i in range(self.num_samples):\r\n",
        "      if self.is_val: # case of validation dataset\r\n",
        "        if i%self.num == self.num-1:\r\n",
        "          k.append(i)\r\n",
        "      else: # case of train dataset\r\n",
        "        if i%self.num != self.num-1:\r\n",
        "          k.append(i)\r\n",
        "\r\n",
        "    if self.shuffle:\r\n",
        "      random.shuffle(k)\r\n",
        "    return iter(k)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.num_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FABQyUDRPPu"
      },
      "source": [
        "# noise\r\n",
        "poisson_noise_n = 0.3\r\n",
        "\r\n",
        "# train\r\n",
        "train_batch_size = 1\r\n",
        "val_batch_size = 1\r\n",
        "\r\n",
        "# load dataset\r\n",
        "train_dataset = three(data_path = '/content/drive/MyDrive/train', transform = transforms.Compose([ToTensor(), shot_noise(poisson_noise_n)]))\r\n",
        "val_dataset = three(data_path = '/content/drive/MyDrive/train', transform = transforms.Compose([ToTensor()]))\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, sampler=num_sampler(train_dataset, is_val=False, shuffle=True))\r\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_batch_size, sampler=num_sampler(val_dataset))\r\n",
        "train_batch = len(train_loader)\r\n",
        "one_epoch_size = 450\r\n",
        "val_size = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1bO0vTPCli4"
      },
      "source": [
        "\"\"\"\r\n",
        "# test one image\r\n",
        "i = 0\r\n",
        "sample = dataset[i]\r\n",
        "print(i, sample['amplified'].shape, sample['mag_factor'], sample['mag_factor'].shape)\r\n",
        "a = sample['frameA']\r\n",
        "print(a)\r\n",
        "a= (a+1)*127.5\r\n",
        "a = a /255\r\n",
        "b= transforms.ToPILImage()(a)\r\n",
        "imshow(b)\r\n",
        "\r\n",
        "# check the batch dataset size\r\n",
        "for i_batch, sample_batched in enumerate(data_loader):\r\n",
        "    print(i_batch, sample_batched['amplified'].size(), sample_batched['mag_factor'].size())\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLT5M2JdMoL5"
      },
      "source": [
        "**Hyperprameters & Device Setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en8dVaLA-ObP"
      },
      "source": [
        "# for exponential decay\r\n",
        "decay_steps = 3000\r\n",
        "lr_decay = 1.0\r\n",
        "\r\n",
        "# for Adam\r\n",
        "betal = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcGzqc7jTpJU"
      },
      "source": [
        "\r\n",
        "**Loss &** **Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6tAdt27TrB5"
      },
      "source": [
        "import torch\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "# set variable\r\n",
        "PATH = '/content/drive/MyDrive/model'\r\n",
        "load_num = 100\r\n",
        "load_name = '/epoch_{}'.format(load_num)\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "is_load = False\r\n",
        "num_epoch = 400\r\n",
        "tex_loss_w = 1.0\r\n",
        "sha_loss_w = 1.0\r\n",
        "\r\n",
        "# load model\r\n",
        "if is_load:\r\n",
        "  model = TheModelClass(*args, **kwargs)\r\n",
        "  optimizer = TheOptimizerClass(*args, **kwargs)\r\n",
        "\r\n",
        "  checkpoint = torch.load(PATH+load_name)\r\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\r\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "  num_epoch -= checkpoint['epoch']\r\n",
        "  train_losses = checkpoint['train_loss']\r\n",
        "  val_losses = checkpoint['val_loss']\r\n",
        "\r\n",
        "# initialize model, criterion\r\n",
        "else:\r\n",
        "  model = magnet().to(device)\r\n",
        "  criterion = torch.nn.L1Loss()\r\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, betas = (betal,0.999), weight_decay = 0, amsgrad=False)\r\n",
        "  train_losses = []\r\n",
        "  val_losses = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu1luPG6TrJf"
      },
      "source": [
        "\r\n",
        "**Training & Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbgEqRsBAdzx"
      },
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/train_loss.txt', 'w') as f1:  \r\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/val_loss.txt', 'w') as f2:\r\n",
        "    for epoch in tqdm(range(num_epoch)):\r\n",
        "        running_loss = 0.0\r\n",
        "        model.train()\r\n",
        "        \r\n",
        "        # train\r\n",
        "        for i, sample in enumerate(train_loader):\r\n",
        "            amplified, frameA, frameB, frameC, amp_factor = sample['amplified'].to(device), sample['frameA'].to(device), sample['frameB'].to(device), sample['frameC'].to(device), sample['mag_factor'].to(device)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            Y, Va, Vb, _, _, Mb, Mb_ = model(amplified, frameA, frameB, frameC, amp_factor)\r\n",
        "            loss = criterion(Y, amplified) + tex_loss_w * criterion(Va, Vb) + sha_loss_w * criterion(Mb, Mb_)\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            running_loss += loss.item()\r\n",
        "\r\n",
        "        # evaluation\r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            val_loss = 0.0\r\n",
        "            for k, sample in enumerate(val_loader):\r\n",
        "                val_amp, val_A, val_B, val_C, val_factor = sample['amplified'].to(device), sample['frameA'].to(device), sample['frameB'].to(device), sample['frameC'].to(device), sample['mag_factor'].to(device)\r\n",
        "                Y, Va, Vb, _, _, Mb, Mb_ = model(val_amp, val_A, val_B, val_C, val_factor)\r\n",
        "                loss = criterion(Y, val_amp) + tex_loss_w * criterion(Va, Vb) + sha_loss_w * criterion(Mb, Mb_)\r\n",
        "                val_loss += loss.item()\r\n",
        "        \r\n",
        "        # result\r\n",
        "        print('[epoch: %d] train_loss: %.3f, val_loss: %.3f'% (epoch + 1, running_loss / one_epoch_size, val_loss / val_size))\r\n",
        "        f1.write('%.3f\\n' % (running_loss / one_epoch_size))\r\n",
        "        f2.write('%.3f\\n' % (val_loss / val_size))\r\n",
        "        train_losses.append(running_loss / one_epoch_size)\r\n",
        "        val_losses.append(val_loss / val_size)\r\n",
        "\r\n",
        "        # save\r\n",
        "        torch.save({'epoch': epoch+1, \r\n",
        "                    'model_state_dict': model.state_dict(), \r\n",
        "                    'optimizer_state_dict': optimizer.state_dict(), \r\n",
        "                    'train_loss':train_losses, \r\n",
        "                    'val_loss':val_losses}, \r\n",
        "                    PATH+'/epoch_{}'.format(epoch+1))\r\n",
        "\r\n",
        "        # for re-train\r\n",
        "        torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f76fEUj8xJPI"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "#Evaluation\r\n",
        "iters = range(0, epoch)\r\n",
        "print(len(train_losses))\r\n",
        "plt.plot(iters, train_losses, 'k', label='Training loss')\r\n",
        "plt.plot(iters, val_losses, 'b', label = 'Validation loss')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.xlabel('iters')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irkzvoqt3Zg8"
      },
      "source": [
        "\r\n",
        "\"\"\"\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "import random\r\n",
        "\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.data.sampler import Sampler\r\n",
        "\r\n",
        "# Dataset 상속\r\n",
        "class CustomDataset(Dataset): \r\n",
        "  def __init__(self):\r\n",
        "    self.x_data = [[1], [2], [3], [4], [5], [6], [7], [8], [9],[10], [11], [12],[13], [14], [15]]\r\n",
        "\r\n",
        "  # 총 데이터의 개수를 리턴\r\n",
        "  def __len__(self): \r\n",
        "    return len(self.x_data)\r\n",
        "\r\n",
        "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\r\n",
        "  def __getitem__(self, idx): \r\n",
        "    x = torch.FloatTensor(self.x_data[idx])\r\n",
        "    return x\r\n",
        "\r\n",
        "class num_sampler(Sampler):\r\n",
        "# Sampling a specific number of multiple-th indices from data.\r\n",
        "  def __init__(self, data, is_val=True, shuffle=False, num=10):\r\n",
        "    self.num_samples = len(data)\r\n",
        "    self.is_val = is_val\r\n",
        "    self.shuffle = shuffle\r\n",
        "    self.num = num\r\n",
        "\r\n",
        "  def __iter__(self):\r\n",
        "    k = []\r\n",
        "    for i in range(self.num_samples):\r\n",
        "      if self.is_val: # case of validation dataset\r\n",
        "        if i%self.num == self.num-1:\r\n",
        "          k.append(i)\r\n",
        "      else: # case of train dataset\r\n",
        "        if i%self.num != self.num-1:\r\n",
        "          k.append(i)\r\n",
        "\r\n",
        "    if self.shuffle:\r\n",
        "      random.shuffle(k)\r\n",
        "    return iter(k)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.num_samples\r\n",
        "\r\n",
        "\r\n",
        "dataset = CustomDataset()\r\n",
        "dataloader = DataLoader(dataset, batch_size=2, sampler=DummySampler(dataset))\r\n",
        "\r\n",
        "# check the batch dataset size\r\n",
        "for i_batch, sample_batched in enumerate(dataloader):\r\n",
        "    print(i_batch, sample_batched)\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "for i_batch, sample_batched in enumerate(dataloader):\r\n",
        "    print(i_batch, sample_batched)\r\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3602x0fBAcCO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW-ph-u6AnQw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}